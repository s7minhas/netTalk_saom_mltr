====================================================================================
====================================================================================
Network Model Data Structure/Time Series of Networks
==
Before we go into the model let me introduce some notation. 

Say that we have g actors in our network. For any pair of actors in the set at a given point in time in our set, our dependent variable is 1 if some sort of event between the two has occurred and zero otherwise. 

Within ERGM based framework such as the SAOM, we can only model dependent variables structured in this way. If we had a continuous or count variable an alternative framework, such as a latent space approach, would need to be chosen.
====================================================================================
====================================================================================

====================================================================================
====================================================================================
Time Series of Networks
==
Now as shown before the way that a series of these dyadic observations are typically organized is in a dataset such as the one shown on the left. When we move to modeling this in the network setting,however, we utilize what are known as adjacency matrices. The diagonals here are by default zero, indicating that a country cannot send an action to it itself, and the cross-sections represent the relationship between a given dyad at time t. 

When we shift to the longitudinal context where we have 1 through n observations of a network, we convert the set of adjacency matrices into a higher dimensional structure known as an array.
====================================================================================
====================================================================================

====================================================================================
====================================================================================
SAOM Assumptions
==
The SAOM framework comes with a number of assumptions. 

1. First, we assume that the actors control their outgoing ties, i.e. they decide to change one of their outgoing ties according to their position in the network, their attributes and the characteristics of the other actors. This means that actors change their ties based on a want to maximize a utility function given the structural constraints of the network.

And to maximize this utility function we have to assume that actors have full knowledge of the whole network, in terms of the existing ties and the attributes that characterize the other actors in the network. 

2. Second, a fundamental assumption of the model regards the distribution of the process that describes network evolution. First of all, we assume that the process of network evolution occurs in continuous time, and specifically, we assume that there is a latent process underlying the network observed and going on between the discrete observation moments,  t1,...,tn. 

3. Additionally, it is assumed that no more than one tie can change at any given microstep. 

As a result of these assumptions, the subsequent changes ("mini-steps") generate an endogenous dynamic context which implies a dependence between the actors over time. This implies strong dependence between what the actors do, and the actors are dependent because they constitute each other's changing environment.

4. The last key assumption here is that, the evolving network being studied is the outcome of a continuous-time Markov chain. This is a very strong assumption that relies on the Markovian property: "The future depends on the present and on the past, only through the present". In our context this mean that for any point in time, the current state of the network determines probabilistically its further evolution, and there are no additional effects of the earlier past. 

% Discuss some consequences of these assumptions:
% Having assumed that the underlying process is a continuous-time Markov chain, network ties represent a state with a tendency to endure over time, rather than a brief event.
====================================================================================
====================================================================================

====================================================================================
====================================================================================
SAOM General Process
==
At its core the goal of the SAOM is to come up with a model that generates a sequence of networks and it adjusts the parameters of the model in such a way that the sequence of networks produced matches the observed sequence of networks.

To begin this process, we condition on the first observation and model the changes that took place to get to the second observation of the network. And we model the movement from one observation period to the other using  microsteps. 

Now each of these micro-steps has a pair of elements: in the first stage an actor is selected from the network and provided with the opportunity to change the existing configuration. 

The second determines the precise change which is made. Here actors make choices about what to change based on their utility function. 

That change then becomes part of the network and thus influences the ministeps of subsequent actors. The total network change between observation moments is simply the accumulation of these ministeps. 

% In this way, the model allows statistical inferences to be drawn even from ties that appear to emerge in a single observation moment. 
This simulation process terminates once the network modified through the series of microsteps resembles the network at t1.
====================================================================================
====================================================================================

====================================================================================
====================================================================================
Rate function
==
The rate function governs the frequency with which actors can be selected to change their ties. In my empirical illustration of this model, I make the simplifying assumption that all actors have the same rate of change between any two given observation moments. 

For all actors independent exponentially distributed waiting times are generated, and the minimum of these is the time for the next microstep. Thus the waiting time until the next microstep by any actor is exponentially distributed with rate parameter g λt, and the conditional probability that it is actor i who gets the opportunity to make a change is 1/g. 

The higher λt is the greater the number of changes that will occur between observation moments. 
====================================================================================
====================================================================================

====================================================================================
====================================================================================
Objective Function
==
The objective function determines the precise tie that an actor makes when he has the opportunity to change. The idea underlying the change determination process is that at a given time t, actor i has the opportunity to modify one of his outgoing ties or maintain his outgoing ties as they are. The purpose of the change determination process is to probabilistically describe the choice of i.

The independent variables in this model are loaded into this first parameter. Each variable in the model corresponds to possible reasons why an actor might want to change a tie. These variables that we load here indicate the actors’ preferences for optimizing their place in the existing network. Here we can include parameterizations of endogenous network characteristics or simple monadic and dyadic variables. 

The beta parameters here just represent the relative weights for each of the independent variables we include and can be interpreted just as the regression coefficients from a typical regression model.

% If βk is equal to 0, it means that the corresponding effect plays no role in the network dynamics. If it assumes positive values, then that parameter increases the probability of a bilateral linkage. Vice versa if the parameter takes negative values.

The last term Ui(t,x,j) of the objective function is a random term accounting for the factors not included in the actor's objective function. And here we will make the choice that the random terms are independent and identically distributed as a  Gumbel distribution.
====================================================================================
====================================================================================

====================================================================================
====================================================================================
Multinomial Choice Model
==
Once an actor is selected by the rate function what they face is a discrete choice problem. Where they are looking across the network and picking another actor with whom they want to change their relationship. In the econometric literature there is wide set of approaches that have been developed to deal with discrete choices. 

Among the models that were formulated, there are the random utility models. They are applied when a decision maker faces a choice between n alternatives. He would obtain a certain level of utility from each alternative, so that he chooses the alternative that provides him the greatest utility. The reason we assumed that the error term follows a gumbel distribution, is because the resulting probabilistic choice can be expressed as a multinomial logit. 

% Similar to multinomial logistic regression, the estimate for a given covariate is the log odds ratio of the respective prob- abilities that i will choose one particular ij tie over another, given that the only difference between the two ties is a one-unit change in the covariate of in- terest  

In short, i evaluates the network and considers the utility of changing its tie to j, relative to the utility of changing its tie to the other actors in the network, and chooses to change its tie to whichever j maximizes that value.

The continuous-time Markov Chain reflects the assumption that network evolution occurs as a series of incremental changes or ministeps by actors in their network ties, of which we only observe the accumulation at discrete moments in time. The intensity matrix is thus the rate at which an actor takes ministeps multiplied by the probability that, if i does take a ministep, it makes a change to the yij tie.
====================================================================================
====================================================================================

====================================================================================
====================================================================================
Parameter Estimation
==
Let θ represent our rate and effect parameters corresponding to the particular specification of the SAOM. Estimating θ allows inferences about whether certain mechanisms drive the network evolution. Estimation is performed conditional on the first observation of the network. Therefore no assumption is necessary about what might have generated that initial network. Thus the estimated parameters refer exclusively to the evolution of the network in our observed time interval.

Several estimation procedures might be used to estimate θ. The initial implementation by Snijders used a method of moments approach, but Bayesian and MLE procedures have also been developed since. Though the MoM approach is still the most widely used. 

The logic of the MoM approach is based on the idea that the expected values of statistics depend on the parameters of the distribution. 

To estimate θ using an MoM approach, we require a p dimensional vector of sufficient statistics (S1,...Sp), one for each of our parameters, and estimates θ by the value \hat{θ} for which the expected values equal their sample counterparts. 

The first step then is setting up some sufficient statistics. The statistics, Sp, are usually chosen using a formal method, however, Snijders argues that this cant be done with the SAOM because of the complexity of the model.

Therefore, the statistics are chosen in a heuristic way. The requirement is that they are sensitive to the parameter θ in the sense that higher values of the parameter θ, will lead to higher expected values of the Sp.

% Another way of thinking about it is that the suitable statistic $S = (S_{1}, \ldots, S_{p})$ of network activity should be chosen such that the expectation of it, $E_{\boldsymbol\theta}(S_{p})$, is a monotonic function of the parameter we weish to estimate, $\boldsymbol\theta_{p}$.

Suitable statistic for rate:
The rate parameter models the frequency at which actors get an opportunity to make a change. When this frequency is higher, the total observed number of changes between two consecutive observations will tend to be higher. Therefore, a relevant statistic for the rate parameter is the Hamming distance between X(tm) and X(tm-1). 

Suitable statistic for beta:
Larger values of the βh parameters should result in larger values of the associated sih network statistic at observation moment tm+1. 

For example, the model will search for a simulated network that has as many closed indirect ties as the observed network.

% The goal of simulation is to locate those βh parameter values that, when a given sh network statistic is summed over all i and m in the network, yields an expected value of sh equal to the observed value.
====================================================================================
====================================================================================

====================================================================================
====================================================================================
Stochastic Approximation Process
==
Thus the moment equations for the estimation of the rate and beta parameters can be summarized as follows: 

This simply represents the sum of deviations between the expected value of the statistics for the random (simulated) networks and the observed networks; 

	zn simply means all of the available data
	θ is our vector of parameters. 
	u(x) is a function that corresponds to appropriately chosen statistics. 

Analytical procedures cannot be applied to the system of moment equations, since no one has come up with an explicit representation for the expected value of the statistics. For this reason a stochastic approximation algorithm is used. Specifically, the SAOM uses a Robbins-Monro MCMC algorithm to search the parameter space and locate the vector θˆ for which the observed and expected values of these statistics are equal, so where gn is equal to zero. 

The randomly sampled parameter values resulting from this approach are then used to construct a sampling distribution, which has been shown to follow a normal distribution, thereby permitting the usual methods of statistical inference.

% First obtain a rough estimate of the partial of the expectation with respect to theta using the score function method. After this we update the initial parameter value using the Newton-Raphson step: θ_1 = θ_0 −a_0 D^{−1}_{0} (S−s)

% Notes on Robbins-Monro:
% The algorithm is an iterative procedure consisting of two main steps:
%	 i) simulations of network evolution for a given value θi of θ;
%	 ii) update of the value of θ according to the multivariate version of the Robbins-Monro step. 
% Finally the estimation holds beta constant at beta hat and performs additional simulations to estimate the covariance matrix using the delta method. 

% The check of convergence of the algorithm is based on the deviations between simulated values of the statistics and their observed values. 
====================================================================================
====================================================================================

Thus to sum up, we are imagining that the way a network changes from t1 to t2 is through a set of ministeps. in each of these ministeps an actor is picked from the network and given the opportunity to change one of their ties. then in the next ministep that change made by the previous actor is a part of the new network. 

thus these ministeps create an endogenous dynamic context through which network interdependencies can play out. And we estimate these parameters essentially by continuing to simulate this process until we find a set of paramters that produce the observed sequence of networks.
